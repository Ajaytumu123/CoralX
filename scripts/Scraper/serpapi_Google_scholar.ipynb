{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search_results.json Is everything that we need at the time of running the script\n",
    "\n",
    "EveryResults_Backup.json Is everything so we never have to rerun the script for the same author ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import KeyToSerpapi\n",
    "\n",
    "def get_all_results(query, max_pages=20):\n",
    "    all_results = []\n",
    "    start = 0\n",
    "    page = 1\n",
    "    everything = []\n",
    "\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\",\n",
    "            \"q\": query,\n",
    "            \"api_key\": KeyToSerpapi.api_key,\n",
    "            \"start\": start,\n",
    "            \"num\": 20  # Request maximum number of results per page\n",
    "        }\n",
    "        try:\n",
    "            search = GoogleSearch(params)\n",
    "            results = search.get_dict()\n",
    "            organic_results = results.get(\"organic_results\", [])\n",
    "            \n",
    "            if not organic_results:\n",
    "                break  # No more results\n",
    "            \n",
    "            for result in organic_results:\n",
    "                publication_info = result.get(\"publication_info\", {})\n",
    "                authors = publication_info.get(\"authors\", [])\n",
    "                result_data = {\n",
    "                    \"resources\": result.get(\"resources\", []),\n",
    "                    \"title\": result.get(\"title\"),\n",
    "                    \"authors\": authors,  # Extract authors from publication_info\n",
    "                    \"snippet\": result.get(\"snippet\"), # Description\n",
    "                }\n",
    "                all_results.append(result_data)\n",
    "                everything.append(result)\n",
    "            \n",
    "            print(f\"Retrieved {len(organic_results)} results from page {page}\")\n",
    "            start += 20\n",
    "            page += 1\n",
    "            time.sleep(2)  # Add a delay to avoid hitting rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_results, everything\n",
    "\n",
    "def file_authors(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        authors = [line.strip() for line in file]\n",
    "    return authors\n",
    "\n",
    "def append_to_json_file(file_path, new_data):\n",
    "    if os.path.exists(file_path):\n",
    "        # Read existing data\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                existing_data = json.load(file)\n",
    "                if not isinstance(existing_data, dict):\n",
    "                    existing_data = {}  # Ensure existing_data is a dictionary\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = {}  # If file is empty or corrupt\n",
    "    else:\n",
    "        existing_data = {}\n",
    "    \n",
    "    # Update existing data with new data\n",
    "    existing_data.update(new_data)\n",
    "    \n",
    "    # Write combined data back to file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(existing_data, file, indent=2)\n",
    "\n",
    "query = \"author:\"\n",
    "file = 'authors.txt' # File with the authors\n",
    "authors = file_authors(file)\n",
    "allFilteredResults = {}\n",
    "allResults = {}\n",
    "\n",
    "for author in authors:\n",
    "    print(f\"Searching for: {author}\")\n",
    "    results, every_result = get_all_results(query + author)\n",
    "    \n",
    "    allFilteredResults[author] = results\n",
    "    allResults[author] = every_result\n",
    "\n",
    "# Define the file paths\n",
    "results_file_path = 'Search_results.json'\n",
    "every_results_file_path = 'EveryResults_Backup.json'\n",
    "\n",
    "try:\n",
    "    # Append results to the JSON files\n",
    "    append_to_json_file(results_file_path, allFilteredResults)\n",
    "    append_to_json_file(every_results_file_path, allResults)\n",
    "\n",
    "    # Calculate the total number of results\n",
    "    total_results = sum(len(results) for results in allFilteredResults.values())\n",
    "    print(f\"Total results retrieved: {total_results}\")\n",
    "    print(f\"Results have been written to {results_file_path}\")\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while writing files: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
