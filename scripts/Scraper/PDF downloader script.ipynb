{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "##########################\n",
    "# PDF Downloader \n",
    "##########################\n",
    "\n",
    "# Directory to save downloaded documents and log file\n",
    "DOWNLOAD_DIR = 'Downloads'\n",
    "LOG_FILE = 'download_log.txt'\n",
    "\n",
    "# Create the download directory if it doesn't exist\n",
    "if not os.path.exists(DOWNLOAD_DIR):\n",
    "    os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "def get_unique_filename(url, original_filename):\n",
    "    \"\"\"Generate a unique filename based on URL, timestamp, and original filename.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Extract the file extension (default to .pdf if not present)\n",
    "    file_extension = os.path.splitext(original_filename)[1]\n",
    "    if not file_extension:\n",
    "        file_extension = '.pdf'\n",
    "    \n",
    "    # Create a base filename from the original, or use 'document' if it's not suitable\n",
    "    base_filename = os.path.splitext(original_filename)[0]\n",
    "    if not base_filename or base_filename == 'pdf':\n",
    "        base_filename = 'document'\n",
    "    \n",
    "    # Construct the unique filename\n",
    "    unique_filename = f\"{base_filename}_{timestamp}{file_extension}\"\n",
    "    return sanitize_filename(unique_filename)\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\/:*?\"<>|]', '', filename)\n",
    "\n",
    "async def download_file(semaphore: asyncio.Semaphore, session: ClientSession, url: str, filename: str):\n",
    "    \"\"\"Download a file from a URL and save it to the specified filename asynchronously with a semaphore limit.\"\"\"\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            print(f\"Attempting to download from URL: {url}\")  # Debug print\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            # Generate a unique filename\n",
    "            unique_filename = get_unique_filename(url, filename)\n",
    "            file_path = os.path.join(DOWNLOAD_DIR, unique_filename)\n",
    "\n",
    "            # Check if the file already exists in the download directory\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"File {unique_filename} already exists, skipping download.\")\n",
    "                return file_path  # Return the path of the existing file\n",
    "\n",
    "            async with session.get(url, headers=headers, timeout=30) as response:\n",
    "                response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "                # Check if the content type is PDF or octet-stream\n",
    "                content_type = response.headers.get('Content-Type', '').lower()\n",
    "                if 'application/pdf' not in content_type and 'application/octet-stream' not in content_type:\n",
    "                    print(f\"Warning: Content-Type is {content_type}\")\n",
    "                    return None  # Stop if it's not a PDF\n",
    "\n",
    "                # Write the file asynchronously\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    while True:\n",
    "                        chunk = await response.content.read(8192)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        f.write(chunk)\n",
    "\n",
    "                # Verify the file size\n",
    "                if os.path.getsize(file_path) == 0:\n",
    "                    os.remove(file_path)\n",
    "                    return f\"Downloaded file is empty: {url}\"\n",
    "\n",
    "                return file_path  # Return the path of the downloaded file\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}. Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "async def process_json(session: ClientSession, semaphore: asyncio.Semaphore, json_data):\n",
    "    \"\"\"Process JSON data to download PDF files from the resources links and log details asynchronously.\"\"\"\n",
    "    log_entries = []  # List to store log entries\n",
    "    tasks = []  # List to store download tasks\n",
    "    \n",
    "    for author, results in json_data.items():\n",
    "        log_entries.append(f'Starting: {author}')\n",
    "        for idx, item in enumerate(results, start=1):\n",
    "            resources = item.get('resources')\n",
    "            if resources:\n",
    "                resources = resources[0]\n",
    "                if resources['file_format'] == 'PDF':\n",
    "                    title = sanitize_filename(item.get('title'))\n",
    "                    PDFLink = resources['link']\n",
    "                    \n",
    "                    # Create a task for the download and log entry\n",
    "                    tasks.append(download_file(semaphore, session, PDFLink, title))\n",
    "                    log_entries.append(f\"{idx}. URL: {PDFLink}\\n   Saved as: {title}\\n\")\n",
    "    \n",
    "    # Download in batches\n",
    "    batch_size = 5\n",
    "    while tasks:\n",
    "        batch = tasks[:batch_size]\n",
    "        tasks = tasks[batch_size:]\n",
    "        \n",
    "        # Wait for the current batch to finish\n",
    "        downloaded_files = await asyncio.gather(*batch)\n",
    "        \n",
    "        # Write log entries to the log file\n",
    "        with open(LOG_FILE, 'w') as log_file:\n",
    "            log_file.writelines(log_entries)\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "##########################\n",
    "# Filter \n",
    "##########################\n",
    "\n",
    "def is_coral_related(text):\n",
    "    prompt = (\n",
    "        \"Determine whether the following text is related to the ocean. \"\n",
    "        \"Only respond with 'Yes' if it is related and 'No' if it is not.\\n\\n\"\n",
    "        f\"Text: \\\"{text}\\\"\\n\\n\"\n",
    "        \"Response:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Or use \"gpt-3.5-turbo\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in marine biology.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0  # Keep responses consistent\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        print(answer + \" - \" + text)\n",
    "        return answer == \"yes\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "async def filter_json_data(data):\n",
    "    # Iterate over each author in the input data\n",
    "    filtered_data = {}\n",
    "\n",
    "    for author, publications in data.items():\n",
    "        filtered_publications = []\n",
    "        \n",
    "        for publication in publications:\n",
    "            # First check if there are PDF resources\n",
    "            resources = [resource for resource in publication.get('resources', []) \n",
    "                       if resource.get('file_format') == 'PDF']\n",
    "            \n",
    "            if not resources:\n",
    "                continue  # Skip if no PDF resources\n",
    "            \n",
    "            text = publication.get('title', '') + '\\n' + publication.get('snippet', '')\n",
    "\n",
    "            # If we get here, we have a PDF and valid date, now check coral-related content\n",
    "            if is_coral_related(text):\n",
    "                filtered_publication = publication.copy()\n",
    "                filtered_publication['resources'] = resources\n",
    "                filtered_publications.append(filtered_publication)\n",
    "\n",
    "        # Only keep the author if they have publications after filtering\n",
    "        if filtered_publications:\n",
    "            filtered_data[author] = filtered_publications\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "##########################\n",
    "# Run\n",
    "##########################\n",
    "\n",
    "async def main():\n",
    "    \n",
    "    # Name of the inputed JSON\n",
    "    json_file_path = 'EveryResults_Backup.json'\n",
    "    \n",
    "    output_file = 'FilteredResults.json'\n",
    "\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            # Load JSON data\n",
    "            json_data = json.load(file)\n",
    "            \n",
    "            # Filter the data asynchronously\n",
    "            filtered_data = await filter_json_data(json_data)\n",
    "            \n",
    "            # Save the filtered data for viewing if needed\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "            # Create an aiohttp session and a semaphore to limit concurrency\n",
    "            semaphore = asyncio.Semaphore(10)  # Limit to 10 concurrent downloads\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                # Process the JSON data to download files asynchronously in batches\n",
    "                downloaded_files = await process_json(session, semaphore, filtered_data)\n",
    "\n",
    "            print(f\"Filtered data has been saved to '{output_file}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {json_file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from file: {json_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if there is an active event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # If event loop is running (e.g., in Jupyter), use asyncio.ensure_future\n",
    "            asyncio.ensure_future(main())\n",
    "        else:\n",
    "            # If no event loop is running, we can run the main function normally\n",
    "            asyncio.run(main())\n",
    "    except RuntimeError as e:\n",
    "        # In case of runtime error with no event loop running\n",
    "        asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
